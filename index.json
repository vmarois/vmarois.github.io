[{"authors":["admin"],"categories":null,"content":"I am currently part of the first class of AI Residents at IBM, advised by Tim Klinger and Murray Campbell. Most recently, my research interests gravitate around compositional generalization in neural models, as well as transfer learning and visual reasoning.\nPreviously, I graduated from Georgia Tech and INSA Lyon with a dual degree in Machine Learning and Computer Engineering.\nI had the opportunity to work with industry-facing and research teams through several internships. I am very interested in translating new research ideas into user-friendly features and products. I enjoy working at the intersection of AI Research and Software Engineering, as I believe both domains benefit from one another.\nJune 2020: I am currently looking for opportunities as a Machine Learning Engineer, ideally in New York but I\u0026rsquo;m open to relocation. Feel free to contact me and have a look at my resume!\n","date":1592265600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1592265600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am currently part of the first class of AI Residents at IBM, advised by Tim Klinger and Murray Campbell. Most recently, my research interests gravitate around compositional generalization in neural models, as well as transfer learning and visual reasoning.\nPreviously, I graduated from Georgia Tech and INSA Lyon with a dual degree in Machine Learning and Computer Engineering.\nI had the opportunity to work with industry-facing and research teams through several internships.","tags":null,"title":"Vincent Marois","type":"authors"},{"authors":["Tim Klinger","Dhaval Adjodah","Vincent Marois","Josh Joseph","Matthew Riemer","Alex 'Sandy' Pentland","Murray Campbell"],"categories":null,"content":"","date":1592265600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592265600,"objectID":"fdc5235ac9fabad966edf82624c686be","permalink":"/publication/compositional-gen/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/compositional-gen/","section":"publication","summary":"A framework to study how neural models generalize in compositional, relational domains.","tags":["deep-learning","neuro-symbolic","pytorch","python"],"title":"A Study of Compositional Generalization in Neural Models","type":"publication"},{"authors":["T.S. Jayram","Vincent Marois","Tomasz Kornuta","Vincent Albouy","Emre Sevgen","Ahmet S. Ozcan"],"categories":null,"content":"","date":1574812800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574812800,"objectID":"5caf4b940eb519a5e710fc5eaa94d379","permalink":"/publication/transfer-learning/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/transfer-learning/","section":"publication","summary":"A proposed Transfer Learning taxonomy for Visual Reasoning and a new State-of-the-Art model.","tags":["deep-learning","visual-reasoning","research","python"],"title":"Transfer Learning in Visual and Relational Reasoning","type":"publication"},{"authors":null,"categories":null,"content":"Summary We propose an in-depth analysis and reimplementation of the Transformer model (Vaswani et al., NIPS 2017). Its non-recurrent behavior and sole use of attention makes it an intriguing model to analyze.\nWe perform a hyper-parameters search, as well as a memory-profiling study, both of these allowing us to successfully train and semantically evaluate the model on the IWSLT TED Translation task. Our experiments further enable us to detail particular insights on the behavior of the model and its training process. This article is aligned with the current question of reproducibility in Deep Learning Research.\nOur findings are available on the main project\u0026rsquo;s webpage.\n","date":1556841600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556841600,"objectID":"f734e2258139d9248136775f09a39ce0","permalink":"/project/transformer/","publishdate":"2019-05-03T00:00:00Z","relpermalink":"/project/transformer/","section":"project","summary":"Learning about the Attention Mechanism and the Transformer Model.","tags":["deep-learning","open-source","pytorch","python"],"title":"The Transformer Model","type":"project"},{"authors":null,"categories":null,"content":"Core concepts When training a model, people write programs which typically follow a similar pattern:\n Loading data samples \u0026amp; instantiating the model, Feeding the model batches of sample-label pairs, which are passed through the model forward pass, Computing the loss as a difference between the predicted labels and the ground truth labels, This error is propagated backwards using backpropagation, Updating the model parameters using an optimizer.  During each iteration, the program can also collect statistics (such as the training / validation loss \u0026amp; accuracy) and optionally save the weights of the resulting model to file.\nThis typical workflow led us to the formalization of the core concepts of the framework:\n Problem: A dataset or a data generator, returning a batch of inputs and ground truth labels used for a model training/validation/test, Model: A trainable model (i.e. a neural network), Worker: A specialized application that instantiates the Problem \u0026amp; Model objects and controls the interactions between them, e.g. during training or inference, Configuration file(s): YAML file(s) containing the parameters of the Problem, Model and training procedure, Experiment: A single run (training \u0026amp; validation or test) of a given Model on a given Problem, using a specific Worker and Configuration file(s).  Aside of the Workers, MI-Prometheus currently offers 2 other types of specialized applications, namely:\n Grid Worker: A specialized application which automates the handling of a number of experiments in parallel. Helper: An application useful from the point of view of a running experiment, but which is independent and external to the Workers.  The general idea here is that the Grid Workers are useful to reproduce research, e.g. when one trains a set of independent models on a set of problems and compare the results. In such a situation, the user can use a Helper to download the required datasets (before training) and/or preprocess them in a specific way (e.g. extract representations), which will reduce the overall time of all experiments.\nYou can read more about MI-Prometheus here.\n","date":1544227200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544227200,"objectID":"9d95f7a72b00c140c17d556c9f0bbd67","permalink":"/project/mi-prometheus/","publishdate":"2018-12-08T00:00:00Z","relpermalink":"/project/mi-prometheus/","section":"project","summary":"Enabling reproducible Machine Learning research.","tags":["deep-learning","open-source","pytorch","python"],"title":"MI-Prometheus","type":"project"},{"authors":["Vincent Marois","T.S. Jayram","Vincent Albouy","Tomasz Kornuta","Younes Bouhadjar","Ahmet S. Ozcan"],"categories":null,"content":"","date":1542240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542240000,"objectID":"d7043affc37bf32d31176d45840fb608","permalink":"/publication/vqa/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/vqa/","section":"publication","summary":"A simplified variant of the MAC model that achieves comparable accuracy, while training faster.","tags":["deep-learning","visual-question-answering","research","open-source","pytorch","python"],"title":"On Transfer Learning using a MAC model variant","type":"publication"},{"authors":["Tomasz Kornuta","Vincent Marois","Ryan L. McAvoy","Younes Bouhadjar","Alexis Asseman","Vincent Albouy","T.S. Jayram","Ahmet S. Ozcan"],"categories":null,"content":"","date":1540771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540771200,"objectID":"a2685952536df619706cce21da37207b","permalink":"/publication/mi-prometheus/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/mi-prometheus/","section":"publication","summary":"The paper presents a PyTorch-based, open-source framework for Machine Learning.","tags":["deep-learning","open-source","pytorch","python"],"title":"Accelerating Machine Learning Research with MI-Prometheus","type":"publication"}]